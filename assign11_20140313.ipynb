{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.1307,),(0.3081,)), # mean value = 0.1307, standard deviation value = 0.3081\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './MNIST'\n",
    "\n",
    "training_set = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)\n",
    "testing_set = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of your training data (must be 10,000) =  10000\n",
      "the number of your testing data (must be 60,000) =  60000\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of your training data (must be 10,000) = \", training_set.__len__()) \n",
    "print(\"the number of your testing data (must be 60,000) = \", testing_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, size_kernel=5):\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # *********************************************************************\n",
    "        # input parameter\n",
    "        #\n",
    "        # data size:\n",
    "        #   mnist   : 28 * 28\n",
    "        # *********************************************************************\n",
    "        self.number_class   = num_classes\n",
    "        self.size_kernel    = size_kernel        \n",
    "        \n",
    "        # *********************************************************************\n",
    "        # feature layer\n",
    "        # *********************************************************************\n",
    "        self.conv1          = nn.Conv2d(1, 20, kernel_size=size_kernel, stride=1, padding=int((size_kernel-1)/2), bias=True)\n",
    "        self.conv2          = nn.Conv2d(20, 50, kernel_size=size_kernel, stride=1, padding=int((size_kernel-1)/2), bias=True)\n",
    "\n",
    "        self.conv_layer1    = nn.Sequential(self.conv1, nn.MaxPool2d(kernel_size=2), nn.ReLU(True))\n",
    "        self.conv_layer2    = nn.Sequential(self.conv2, nn.MaxPool2d(kernel_size=2), nn.ReLU(True))\n",
    "\n",
    "        self.feature        = nn.Sequential(self.conv_layer1, self.conv_layer2)\n",
    "        \n",
    "        # *********************************************************************\n",
    "        # classifier layer\n",
    "        # *********************************************************************\n",
    "        self.fc1        = nn.Linear(50*7*7, 50, bias=True)\n",
    "        self.fc2        = nn.Linear(50, num_classes, bias=True)\n",
    "\n",
    "        self.fc_layer1  = nn.Sequential(self.fc1, nn.ReLU(True))\n",
    "        self.fc_layer2  = nn.Sequential(self.fc2, nn.ReLU(True))\n",
    "\n",
    "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2)\n",
    "        \n",
    "        self._initialize_weight()        \n",
    "        \n",
    "    def _initialize_weight(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                \n",
    "                nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
    "\n",
    "                if m.bias is not None:\n",
    "\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "\n",
    "                nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2))\n",
    "                \n",
    "                if m.bias is not None:\n",
    "\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "l_train = torch.utils.data.DataLoader(dataset=training_set, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "l_test = torch.utils.data.DataLoader(dataset=testing_set, batch_size = batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "criterion = nn.CrossEntropyLoss() #change criterion\n",
    "classifier = MyModel().to(device)\n",
    "alph = 1e-3 #change learning rate\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=alph) #change optimizer\n",
    "training_epochs = 80 #change epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_Liters = []; tr_Aiters = []\n",
    "te_Liters = []; te_Aiters = []\n",
    "tr_size = len(l_train); te_size = len(l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  10\n",
      "epoch:  20\n",
      "epoch:  30\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    tr_avg_loss=0\n",
    "    tr_avg_acc=0\n",
    "    te_avg_loss=0\n",
    "    te_avg_acc=0\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"epoch: \", epoch)\n",
    "        \n",
    "    for x, y in l_train:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = classifier(x)\n",
    "        tr_loss = criterion(y_pred, y)\n",
    "        tr_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tr_avg_loss += tr_loss.item() / tr_size\n",
    "        fin_pred = torch.argmax(y_pred, 1) == y\n",
    "        tr_acc = fin_pred.float().mean()\n",
    "        tr_avg_acc += tr_acc.item() / tr_size\n",
    "        \n",
    "        del tr_loss\n",
    "        del y_pred #이거 지울 준비\n",
    "        \n",
    "    with torch.no_grad(): #이거 지울 준비\n",
    "        for x, y in l_test:\n",
    "            y_pred = classifier(x)\n",
    "            te_loss = criterion(y_pred,y)\n",
    "            \n",
    "            te_avg_loss += te_loss.item() / te_size\n",
    "            fin_pred = torch.argmax(y_pred, 1) == y\n",
    "            te_acc = fin_pred.float().mean()\n",
    "            te_avg_acc += te_acc.item() / te_size\n",
    "            \n",
    "            del te_loss\n",
    "            del y_pred\n",
    "            \n",
    "    tr_Liters.append(tr_avg_loss); tr_Aiters.append(tr_avg_acc)\n",
    "    te_Liters.append(te_avg_loss); te_Aiters.append(te_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#바꾸자\n",
    "loss_df = pd.DataFrame(data=np.array([[tr_Liters[-1]], [te_Liters[-1]]]), index=['training', 'testing'], columns=[' '])\n",
    "loss_df.columns.name = \"loss\"\n",
    "loss_df[' '] = loss_df[' '].map('{:,.5f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(data=np.array([[round(tr_Aiters[-1], 5)], [round(te_Aiters[-1], 5)]]),\n",
    "index=['training', 'testing'],\n",
    "columns=[' '])\n",
    "accuracy_df.columns.name = \"accuracy\"\n",
    "accuracy_df[' '] = accuracy_df[' '].map('{:,.5f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(8, 8))\n",
    "plt.plot(np.array(range(training_epochs)), tr_Liters, c='red', label='train loss') \n",
    "plt.plot(np.array(range(training_epochs)), te_Liters, c='blue', label='test loss') plt.title('loss')\n",
    "plt.xticks(range(0, training_epochs, 10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
