{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data with numpy\n",
    "data_train  = np.loadtxt('training.txt', delimiter=',')\n",
    "data_test   = np.loadtxt('testing.txt', delimiter=',')\n",
    "\n",
    "# number of training data\n",
    "number_data_train   = data_train.shape[0] \n",
    "number_data_test    = data_test.shape[0]\n",
    "\n",
    "# training data\n",
    "x1_train            = data_train[:,0] # feature 1\n",
    "x2_train            = data_train[:,1] # feature 2\n",
    "idx_class0_train    = (data_train[:,2]==0) # index of class0\n",
    "idx_class1_train    = (data_train[:,2]==1) # index of class1\n",
    "\n",
    "# testing data\n",
    "x1_test             = data_test[:,0] # feature 1\n",
    "x2_test             = data_test[:,1] # feature 2\n",
    "idx_class0_test     = (data_test[:,2]==0) # index of class0\n",
    "idx_class1_test     = (data_test[:,2]==1) # index of class1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def f_pred(X, w):\n",
    "    return sigmoid(X.dot(w))\n",
    "\n",
    "def loss_logreg(y_pred,y, w, lda): \n",
    "    n = len(y)\n",
    "    reg_term = (lda/2) * np.sum(w**2) \n",
    "    loss = np.mean((-y.T).dot(np.log(y_pred)) - (1 - y).T.dot(np.log(1 - y_pred + 1e-5)))\n",
    "    return loss + reg_term\n",
    "\n",
    "def grad_loss(y_pred, y, X, w, lda):\n",
    "    n = len(y)\n",
    "    reg_term = lda * w\n",
    "    grad = X.T.dot(y_pred - y) * 2 / n\n",
    "    return grad + reg_term\n",
    "\n",
    "# gradient descent function definition\n",
    "def grad_desc(X, y , w_init, tau, max_iter, lda):\n",
    "\n",
    "    L_iters = np.zeros([max_iter]) # record the loss values\n",
    "    w = w_init # initialization\n",
    "    for i in range(max_iter): # loop over the iterations\n",
    "        y_pred = f_pred(X, w) # linear predicition function   \n",
    "        grad_f =  grad_loss(y_pred, y, X, w, lda) # gradient of the loss  \n",
    "        w =  w - (tau * grad_f) # update rule of gradient descent  \n",
    "        L_iters[i] = loss_logreg(y_pred, y, w, lda) # save the current loss value \n",
    "        \n",
    "    return w, L_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
